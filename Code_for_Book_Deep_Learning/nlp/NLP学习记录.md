# 第 1 章

## 第1节





# 第 2 章

## 第1节





# 第章

## 第1节





# 第 4 章

## 第1节



# 第 5 章

前馈神经网络指网络传播方向是单向的网络，不适用于处理时序数据。

处理时序数据需要RNN(Recurent Neural Network)

## 第1节 概率和语言模型

* word2vec 的CBOW模型

  > 根据上下文预测目标词; 获得编码了单词含义信息的单词的分布式表示

* 语言模型

  > 给出了单词序列发生的概率
  >
  > * 联合概率
  >
  >   > 多个事件一起发生的概率
  >
  > * 概率的乘法定理
  >
  >   > *P*(*A,B*) = *P*(*A**|**B*)*P*(*B*)
  >
  > * 马尔可夫性
  >
  >   > 是指未来的状态仅依存于当前状态。此外，当某个事件的概率仅取决于其前面的 *N* 个事件时，称为“*N* 阶马尔可夫链”。

## 第2节 RNN

* 存在环路

  > 各时刻的RNN层接收传给该层的输入以及上一RNN层的输出

* Backpropagation Through Time（简称 BPTT)

  > 按时间顺序展开的神经网络的误差反向传播法

* Truncated BPTT（截断的 BPTT）

  > 在处理长时序数据时，通常的做法是将网络连接截成适当的长度。具体来说，就是将时间轴方向上过长的网络在合适的位置进行截断，从而创建多个小型网络，然后对截出来的小型网络执行误差反向传播法

## 第3节 RNN的实现



## 第 4 节 处理时序数据的层的实现



## 第5节 RNNLM的学习和评价

> 分叉度
>
> > 指下一个可以选择的选项的数量（下一个可能出现的单词的候选个数）



# 第 6 章

## 第 1 节  RNN的问题

	* 不擅长学习时序数据的长期依赖关系，是因为BPTT容易发生梯度消失和梯度爆炸的问题。
	* 梯度消失

​		>> 

 * 梯度爆炸

## 第 2 节 梯度消失







# 2022.11.22

> * 简单版word2vec 模型的实现
>
>   > 主要步骤
>
>   * 
>
> * 高速版word2vec模型的实现
>
>   > 解决简单版word2vec的两个瓶颈
>   >
>   > * 输入层one-hot表示和输出入层权重的乘积
>   > * 中间层和输出层权重的乘积以及softmax的计算

# 2022.11.23

> 基础知识回顾
>
> * softmax 函数
> * sigmoid函数
> * 交叉熵误差函数
> * Affine层
> * MatMul层
> * SoftmaxWithLoss层
> * SigmoidWithLoss层
> * TwoLayerNet
>
> 